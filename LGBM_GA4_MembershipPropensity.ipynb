{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e294d8-4c3d-43af-b29c-2f15c0c0c898",
   "metadata": {},
   "source": [
    "# Explanation of the Script:\n",
    "\n",
    "1.  **Synthetic Data Generation:**\n",
    "    * Creates a `pandas.DataFrame` with `num_customers` rows.\n",
    "    * Includes columns for `age`, `gender`, `region`, various `GA4`-like online metrics (`avg_page_views`, `avg_session_duration`, `num_product_views`, `num_add_to_carts`, `num_online_purchases`, `online_channel`, `has_app_installed`), and simulated `in-store` data (`avg_weekly_spend_instore`, `num_weekly_visits_instore`, `fav_product_category`, `is_near_store`).\n",
    "    * The `is_member` target variable is generated with a synthetic relationship, where certain features (including being 18-30 and having specific online/in-store behaviors) increase the likelihood of being a member. This simulates the real-world scenario where underlying patterns drive membership.\n",
    "\n",
    "2.  **Data Preprocessing:**\n",
    "    * **Categorical Encoding:** `pd.get_dummies` is used for one-hot encoding of categorical features. LightGBM can handle categorical features natively, but one-hot encoding is a common and explicit way to prepare them.\n",
    "    * **Train-Test Split:** The data is split into training (80%) and testing (20%) sets using `train_test_split`. `stratify=y` is used to ensure that the proportion of members in both the training and testing sets is similar to the overall dataset, which is important for imbalanced datasets.\n",
    "\n",
    "3.  **LightGBM Model Training:**\n",
    "    * An `lgb.LGBMClassifier` is initialized. Key parameters like `objective='binary'` (for binary classification) and `metric='auc'` (Area Under the ROC Curve, a robust metric for classification) are set.\n",
    "    * `n_estimators` (number of trees) and `learning_rate` (shrinkage) are fundamental LightGBM parameters.\n",
    "    * The model is trained using `lgbm_clf.fit(X_train, y_train)`.\n",
    "\n",
    "4.  **Model Evaluation:**\n",
    "    * `predict_proba` is used to get the probability of membership for each customer in the test set.\n",
    "    * Standard classification metrics are calculated: `roc_auc_score`, `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, `confusion_matrix`, and `classification_report`. These metrics provide a comprehensive view of how well the model performs.\n",
    "\n",
    "5.  **Feature Importance Analysis:**\n",
    "    * LightGBM models provide `feature_importances_`, which indicate how much each feature contributed to the model's predictions. This is crucial for interpretability and deriving business insights.\n",
    "    * A bar plot visualizes the top features, showing which aspects of customer behavior or demographics are most predictive of membership.\n",
    "\n",
    "6.  **Simulating Targeting 18-30 Year Olds:**\n",
    "    * The script filters the test set to isolate customers aged 18-30.\n",
    "    * It then uses the trained model to predict membership propensity scores specifically for this age group.\n",
    "    * A `quantile` is used to define a \"high-propensity\" threshold (e.g., the top 20% most likely members within the 18-30 age group).\n",
    "    * Finally, it prints out characteristics and descriptive statistics of these high-propensity 18-30 year olds. This is where you'd derive actionable insights for marketing (e.g., \"This segment frequently browses 'Ready Meals' and comes from 'Social' media, so target them with social ads highlighting meal deal member prices\").\n",
    "\n",
    "This script provides a solid foundation for demonstrating your understanding of building and applying a LightGBM model for customer targeting in a data science interview. Remember to explain each section and its purpose, especially how the feature importances and propensity scores translate into actionable business strategies.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb8474-3186-4a68-a440-b647936c0372",
   "metadata": {},
   "source": [
    "### --- Step 1. Synthetic Data Generation ---\n",
    "* Create a synthetic dataset that mimics customer data for membership prediction.\n",
    "* Features will include online behavior (GA4-like), in-store behavior, and demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea92e373-25fe-4bba-8242-e132b24ce797",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.15.2)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91e3f399-1dda-430c-be4f-44afdb02dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthetic Data Head ---\n",
      "   age  gender      region  avg_page_views  avg_session_duration  \\\n",
      "0   56  Female       North              23                   289   \n",
      "1   69  Female  South West              25                   223   \n",
      "2   46    Male      London              20                   369   \n",
      "3   32    Male    Midlands              20                   434   \n",
      "4   60    Male  South East              13                   329   \n",
      "\n",
      "   num_product_views  num_add_to_carts  num_online_purchases  online_channel  \\\n",
      "0                  3                 4                     1          Direct   \n",
      "1                  9                 1                     0        Referral   \n",
      "2                  5                 6                     0  Organic Search   \n",
      "3                  7                 4                     0          Social   \n",
      "4                 10                 2                     1          Social   \n",
      "\n",
      "   has_app_installed  avg_weekly_spend_instore  num_weekly_visits_instore  \\\n",
      "0                  0                     32.50                          1   \n",
      "1                  0                     29.75                          3   \n",
      "2                  0                      1.23                          3   \n",
      "3                  0                     14.25                          3   \n",
      "4                  0                     20.66                          0   \n",
      "\n",
      "  fav_product_category  is_near_store  is_member  \n",
      "0               Drinks              1        0.0  \n",
      "1                Dairy              1        0.0  \n",
      "2        Fresh Produce              1        0.0  \n",
      "3          Ready Meals              1        0.0  \n",
      "4               Bakery              1        0.0  \n",
      "\n",
      "--- Membership Distribution ---\n",
      "is_member\n",
      "0.0    0.76338\n",
      "1.0    0.23662\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Synthetic Data Generation ---\n",
    "# We'll create a synthetic dataset that mimics customer data for membership prediction.\n",
    "# Features will include online behavior (GA4-like), in-store behavior, and demographics.\n",
    "\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "num_customers = 100000\n",
    "\n",
    "# Demographic features\n",
    "ages = np.random.randint(18, 70, num_customers)\n",
    "genders = np.random.choice(['Male', 'Female', 'Other'], num_customers, p=[0.48, 0.50, 0.02])\n",
    "regions = np.random.choice(['London', 'North', 'Midlands', 'South East', 'South West'], num_customers, p=[0.25, 0.20, 0.20, 0.20, 0.15])\n",
    "\n",
    "# GA4-like online shopping data\n",
    "avg_page_views = np.random.normal(loc=15, scale=5, size=num_customers).round().astype(int)\n",
    "avg_session_duration = np.random.normal(loc=300, scale=100, size=num_customers).round().astype(int) # seconds\n",
    "num_product_views = np.random.normal(loc=10, scale=4, size=num_customers).round().astype(int)\n",
    "num_add_to_carts = np.random.normal(loc=2, scale=1.5, size=num_customers).round().astype(int)\n",
    "num_online_purchases = np.random.normal(loc=0.5, scale=1, size=num_customers).round().astype(int)\n",
    "online_channel = np.random.choice(['Direct', 'Organic Search', 'Paid Search', 'Social', 'Referral'], num_customers, p=[0.2, 0.25, 0.2, 0.15, 0.2])\n",
    "has_app_installed = np.random.choice([0, 1], num_customers, p=[0.6, 0.4])\n",
    "\n",
    "# In-store transaction data (from non-members who might become members, or proxy from similar customers)\n",
    "avg_weekly_spend_instore = np.random.normal(loc=20, scale=10, size=num_customers).round(2)\n",
    "num_weekly_visits_instore = np.random.normal(loc=1.5, scale=1, size=num_customers).round().astype(int)\n",
    "fav_product_category = np.random.choice(['Fresh Produce', 'Bakery', 'Ready Meals', 'Frozen', 'Dairy', 'Drinks'], num_customers, p=[0.2, 0.15, 0.2, 0.1, 0.15, 0.2])\n",
    "is_near_store = np.random.choice([0, 1], num_customers, p=[0.3, 0.7]) # Proxy for geographic proximity\n",
    "\n",
    "# Ensure non-negative values for counts\n",
    "avg_page_views[avg_page_views < 0] = 0\n",
    "avg_session_duration[avg_session_duration < 0] = 0\n",
    "num_product_views[num_product_views < 0] = 0\n",
    "num_add_to_carts[num_add_to_carts < 0] = 0\n",
    "num_online_purchases[num_online_purchases < 0] = 0\n",
    "num_weekly_visits_instore[num_weekly_visits_instore < 0] = 0\n",
    "avg_weekly_spend_instore[avg_weekly_spend_instore < 0] = 0.0\n",
    "\n",
    "# Target variable: is_member (binary: 1 for member, 0 for non-member)\n",
    "# We'll create a synthetic relationship where certain features increase membership likelihood.\n",
    "# For example, higher online engagement, more in-store visits, and being in the 18-30 age group might increase propensity to become a member.\n",
    "base_propensity = 0.1 # Overall low membership rate for non-members\n",
    "is_member = np.zeros(num_customers)\n",
    "\n",
    "for i in range(num_customers):\n",
    "    propensity = base_propensity\n",
    "    \n",
    "    # Influence of age: 18-30 year olds might have a slightly higher or lower base propensity\n",
    "    if 18 <= ages[i] <= 30:\n",
    "        propensity += 0.08 # Slightly higher propensity for this age group\n",
    "        if online_channel[i] == 'Social':\n",
    "            propensity += 0.05 # Social channel users in this age group\n",
    "    \n",
    "    # Online engagement influence\n",
    "    if avg_page_views[i] > 20:\n",
    "        propensity += 0.05\n",
    "    if num_add_to_carts[i] > 3:\n",
    "        propensity += 0.07\n",
    "    if num_online_purchases[i] > 0:\n",
    "        propensity += 0.10 # Already purchasing online, higher propensity\n",
    "\n",
    "    # In-store engagement influence\n",
    "    if num_weekly_visits_instore[i] > 2:\n",
    "        propensity += 0.06\n",
    "    if avg_weekly_spend_instore[i] > 30:\n",
    "        propensity += 0.08\n",
    "    if fav_product_category[i] in ['Ready Meals', 'Fresh Produce'] and 18 <= ages[i] <= 30:\n",
    "        propensity += 0.04 # Specific product categories for young adults\n",
    "\n",
    "    # Proximity to store\n",
    "    if is_near_store[i] == 1:\n",
    "        propensity += 0.03\n",
    "\n",
    "    # Random noise to make it less deterministic\n",
    "    propensity = max(0, min(1, propensity + np.random.uniform(-0.05, 0.05)))\n",
    "    \n",
    "    if np.random.rand() < propensity:\n",
    "        is_member[i] = 1\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'age': ages,\n",
    "    'gender': genders,\n",
    "    'region': regions,\n",
    "    'avg_page_views': avg_page_views,\n",
    "    'avg_session_duration': avg_session_duration,\n",
    "    'num_product_views': num_product_views,\n",
    "    'num_add_to_carts': num_add_to_carts,\n",
    "    'num_online_purchases': num_online_purchases,\n",
    "    'online_channel': online_channel,\n",
    "    'has_app_installed': has_app_installed,\n",
    "    'avg_weekly_spend_instore': avg_weekly_spend_instore,\n",
    "    'num_weekly_visits_instore': num_weekly_visits_instore,\n",
    "    'fav_product_category': fav_product_category,\n",
    "    'is_near_store': is_near_store,\n",
    "    'is_member': is_member\n",
    "})\n",
    "\n",
    "print(\"--- Synthetic Data Head ---\")\n",
    "print(data.head())\n",
    "print(\"\\n--- Membership Distribution ---\")\n",
    "print(data['is_member'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9bc0c-06b0-4f9e-af46-1fe03cd6191d",
   "metadata": {},
   "source": [
    "***\n",
    "### --- Step 2. Data Preprocessing ---\n",
    "1. Define catergorical and numerical features\n",
    "2. Use one-hot encoding for categorical features\n",
    "3. Define independent and dependent variable\n",
    "4. Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f08734a-df43-4bb4-bb90-8e9e914536f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (80000, 25)\n",
      "Testing data shape: (20000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical features\n",
    "categorical_features = ['gender', 'region', 'online_channel', 'fav_product_category']\n",
    "numerical_features = [col for col in data.columns if col not in categorical_features + ['is_member']]\n",
    "\n",
    "# LightGBM can handle categorical features natively if specified.\n",
    "# For simplicity and explicit demonstration, we'll use one-hot encoding here.\n",
    "# Alternatively, you can pass `categorical_feature` argument to LightGBM.\n",
    "data_processed = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data_processed.drop('is_member', axis=1)\n",
    "y = data_processed['is_member']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de46a2-cd04-4e9f-81e2-ef5d9063580c",
   "metadata": {},
   "source": [
    "***\n",
    "### --- Step 3. Using GridSearchCV to Tune LightGBM ---\n",
    "1. Define the LightGBM Classifier\n",
    "2. Define the parameter grid to search\n",
    "3. Define the scoring metric (roc_auc for classification)\n",
    "4. Set up GridSearchCV\n",
    "5. Get parameters and best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44c03119-0a39-43e3-b847-6c9a507cb4a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting GridSearchCV for Hyperparameter Tuning ---\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   1.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   5.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   6.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   6.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   6.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   1.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   3.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   5.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   7.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   7.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   1.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   3.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   3.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   4.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   5.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   6.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   8.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.7s[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   3.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   5.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   6.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   6.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   6.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   1.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   3.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   5.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   7.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   3.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   7.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   6.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   6.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   7.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   7.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   3.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   5.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   9.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.1s[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   1.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   3.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   5.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   6.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   4.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   3.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   4.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   5.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   3.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   3.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   5.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   3.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   5.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   3.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   4.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   4.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   6.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   6.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   2.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   4.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   4.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   5.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   6.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   3.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   4.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   5.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   7.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   3.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   3.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   4.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   4.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   4.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   3.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   4.4s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0; total time=   7.2s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   5.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   6.9s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   2.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.5s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   4.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0; total time=   4.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   4.8s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0; total time=   4.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   4.7s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0, reg_lambda=0.1; total time=   7.1s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1; total time=   6.0s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0; total time=   2.3s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0, reg_lambda=0.1; total time=   2.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 15144, number of negative: 48856\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=20, reg_alpha=0.1, reg_lambda=0.1; total time=   2.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 18930, number of negative: 61070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 682\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236625 -> initscore=-1.171273\n",
      "[LightGBM] [Info] Start training from score -1.171273\n",
      "GridSearchCV complete.\n",
      "\n",
      "--- Best Parameters Found by GridSearchCV ---\n",
      "{'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 100, 'num_leaves': 20, 'reg_alpha': 0, 'reg_lambda': 0}\n",
      "\n",
      "--- Best ROC AUC Score on Cross-Validation ---\n",
      "Best CV Score: 0.6300\n"
     ]
    }
   ],
   "source": [
    "# Define the LightGBM Classifier\n",
    "# Use a basic setup for the classifier, as GridSearchCV will tune the specific parameters.\n",
    "lgbm = lgb.LGBMClassifier(objective='binary', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# This is a small grid for demonstration. In practice, you might try wider ranges\n",
    "# or more parameters, but be mindful of computational cost.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         # Number of boosting rounds (trees)\n",
    "    'learning_rate': [0.05, 0.1],      # Step size shrinkage\n",
    "    'num_leaves': [20, 31],            # Max number of leaves in one tree\n",
    "    'max_depth': [-1, 10],             # Max depth of a tree (-1 means no limit)\n",
    "    'reg_alpha': [0, 0.1],             # L1 regularization\n",
    "    'reg_lambda': [0, 0.1]             # L2 regularization\n",
    "}\n",
    "\n",
    "# Define the scoring metric. For classification, 'roc_auc' is often preferred\n",
    "# especially with imbalanced datasets.\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# verbose=2 provides more output during the search\n",
    "# n_jobs=-1 uses all available CPU cores for parallel processing\n",
    "grid_search = GridSearchCV(estimator=lgbm,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "print(\"\\n--- Starting GridSearchCV for Hyperparameter Tuning ---\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"GridSearchCV complete.\")\n",
    "\n",
    "# --- 4. Get Best Parameters and Best Score ---\n",
    "print(\"\\n--- Best Parameters Found by GridSearchCV ---\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\n--- Best ROC AUC Score on Cross-Validation ---\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d47df06f-12a9-4f3c-8489-57716689201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuned Model Evaluation on Unseen Test Set ---\n",
      "ROC AUC Score (Tuned Model): 0.6242\n",
      "Accuracy (Tuned Model): 0.7636\n",
      "Precision (Tuned Model): 0.5909\n",
      "Recall (Tuned Model): 0.0027\n",
      "F1-Score (Tuned Model): 0.0055\n",
      "\n",
      "Confusion Matrix (Tuned Model):\n",
      "[[15259     9]\n",
      " [ 4719    13]]\n",
      "\n",
      "Classification Report (Tuned Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      1.00      0.87     15268\n",
      "         1.0       0.59      0.00      0.01      4732\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.68      0.50      0.44     20000\n",
      "weighted avg       0.72      0.76      0.66     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Use the Best Model for Final Evaluation ---\n",
    "best_lgbm_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict probabilities on the test set using the best model\n",
    "y_pred_proba_tuned = best_lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the tuned model on the unseen test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "auc_score_tuned = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "y_pred_tuned = (y_pred_proba_tuned >= 0.5).astype(int) # Using default 0.5 threshold for classification metrics\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(\"\\n--- Tuned Model Evaluation on Unseen Test Set ---\")\n",
    "print(f\"ROC AUC Score (Tuned Model): {auc_score_tuned:.4f}\")\n",
    "print(f\"Accuracy (Tuned Model): {accuracy_tuned:.4f}\")\n",
    "print(f\"Precision (Tuned Model): {precision_tuned:.4f}\")\n",
    "print(f\"Recall (Tuned Model): {recall_tuned:.4f}\")\n",
    "print(f\"F1-Score (Tuned Model): {f1_tuned:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Tuned Model):\")\n",
    "print(confusion_matrix(y_test, y_pred_tuned))\n",
    "print(\"\\nClassification Report (Tuned Model):\")\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36faa638-f315-47b3-8339-4beaa90b187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Feature Importances (Tuned Model) ---\n",
      "                      feature  importance\n",
      "7    avg_weekly_spend_instore         366\n",
      "1              avg_page_views         243\n",
      "0                         age         221\n",
      "2        avg_session_duration         215\n",
      "4            num_add_to_carts         172\n",
      "8   num_weekly_visits_instore         139\n",
      "3           num_product_views         135\n",
      "5        num_online_purchases          95\n",
      "9               is_near_store          86\n",
      "19      online_channel_Social          50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 6. Feature Importance Analysis (from the best model) ---\n",
    "feature_importances_tuned = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_lgbm_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 10 Feature Importances (Tuned Model) ---\")\n",
    "print(feature_importances_tuned.head(10))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_tuned.head(15))\n",
    "plt.title('LightGBM Feature Importances (Tuned Model)')\n",
    "plt.xlabel('Importance (higher = more influential)')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2463e937-4ed6-45ae-94b4-76347d39cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Simulating Targeting 18-30 Year Olds (Tuned Model) ---\n",
      "\n",
      "Total 18-30 year olds in test set: 4864\n",
      "\n",
      "Sample of 18-30 year olds with predicted membership propensity (Tuned Model):\n",
      "       age  online_channel  avg_weekly_spend_instore  membership_propensity  \\\n",
      "11046   25          Social                     12.55               0.398042   \n",
      "21242   18        Referral                     45.30               0.446696   \n",
      "85235   26          Social                     21.77               0.416179   \n",
      "72043   28  Organic Search                      9.94               0.201573   \n",
      "72864   19  Organic Search                     19.33               0.247302   \n",
      "25594   22        Referral                     13.07               0.214500   \n",
      "29365   19  Organic Search                     23.73               0.444536   \n",
      "25281   29     Paid Search                      7.98               0.325358   \n",
      "32895   27  Organic Search                     17.66               0.397173   \n",
      "28025   21  Organic Search                     10.28               0.295652   \n",
      "\n",
      "       is_actual_member  \n",
      "11046               0.0  \n",
      "21242               0.0  \n",
      "85235               1.0  \n",
      "72043               0.0  \n",
      "72864               0.0  \n",
      "25594               0.0  \n",
      "29365               1.0  \n",
      "25281               1.0  \n",
      "32895               1.0  \n",
      "28025               0.0  \n",
      "\n",
      "Number of 'high-propensity' 18-30 year olds (top 20% by score, Tuned Model): 973\n",
      "Propensity threshold for 'high-propensity' 18-30 year olds (Tuned Model): 0.3735\n",
      "\n",
      "Characteristics of a sample of 'high-propensity' 18-30 year olds (Tuned Model):\n",
      "       age  online_channel  avg_weekly_spend_instore  \\\n",
      "11046   25          Social                     12.55   \n",
      "21242   18        Referral                     45.30   \n",
      "85235   26          Social                     21.77   \n",
      "29365   19  Organic Search                     23.73   \n",
      "32895   27  Organic Search                     17.66   \n",
      "\n",
      "       num_weekly_visits_instore fav_product_category  membership_propensity  \\\n",
      "11046                          2        Fresh Produce               0.398042   \n",
      "21242                          3        Fresh Produce               0.446696   \n",
      "85235                          2        Fresh Produce               0.416179   \n",
      "29365                          3                Dairy               0.444536   \n",
      "32895                          2                Dairy               0.397173   \n",
      "\n",
      "       is_actual_member  \n",
      "11046               0.0  \n",
      "21242               0.0  \n",
      "85235               1.0  \n",
      "29365               1.0  \n",
      "32895               1.0  \n",
      "\n",
      "Descriptive statistics for 'high-propensity' 18-30 year olds (Tuned Model):\n",
      "       avg_page_views  avg_session_duration  num_add_to_carts  \\\n",
      "count      973.000000            973.000000        973.000000   \n",
      "mean        15.774923            302.754368          2.594039   \n",
      "std          5.304873             99.209646          1.580047   \n",
      "min          0.000000              0.000000          0.000000   \n",
      "25%         12.000000            237.000000          1.000000   \n",
      "50%         16.000000            300.000000          3.000000   \n",
      "75%         20.000000            374.000000          4.000000   \n",
      "max         32.000000            602.000000          7.000000   \n",
      "\n",
      "       avg_weekly_spend_instore  num_weekly_visits_instore  \n",
      "count                973.000000                 973.000000  \n",
      "mean                  24.871932                   1.791367  \n",
      "std                   11.505894                   1.052455  \n",
      "min                    0.000000                   0.000000  \n",
      "25%                   16.600000                   1.000000  \n",
      "50%                   27.070000                   2.000000  \n",
      "75%                   33.100000                   3.000000  \n",
      "max                   59.740000                   5.000000  \n",
      "\n",
      "Most common online channels for 'high-propensity' 18-30 year olds (Tuned Model):\n",
      "online_channel\n",
      "Social            0.283659\n",
      "Organic Search    0.221994\n",
      "Paid Search       0.178828\n",
      "Referral          0.167523\n",
      "Direct            0.147996\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Most common favorite product categories for 'high-propensity' 18-30 year olds (Tuned Model):\n",
      "fav_product_category\n",
      "Fresh Produce    0.277492\n",
      "Ready Meals      0.230216\n",
      "Drinks           0.175745\n",
      "Bakery           0.122302\n",
      "Dairy            0.114080\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Marketing Actionable Insights (Tuned Model) ---\n",
      "Based on this tuned model simulation, Co-op could refine their strategy:\n",
      "1. Target 18-30 year olds with a membership propensity score of 0.37 or higher.\n",
      "2. Focus marketing messages for this segment on benefits appealing to their likely interests (e.g., 'Ready Meals' discounts, sustainability, or community initiatives, depending on what the model's features indicate).\n",
      "3. Prioritise digital channels like 'Social' for reaching this segment.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Simulate Targeting 18-30 Year Olds (using the best model) ---\n",
    "print(\"\\n--- Simulating Targeting 18-30 Year Olds (Tuned Model) ---\")\n",
    "\n",
    "test_data_original = data.loc[X_test.index]\n",
    "young_adults_test = test_data_original[(test_data_original['age'] >= 18) & (test_data_original['age'] <= 30)]\n",
    "\n",
    "if not young_adults_test.empty:\n",
    "    X_young_adults_test = X_test.loc[young_adults_test.index]\n",
    "    young_adults_propensity_tuned = best_lgbm_model.predict_proba(X_young_adults_test)[:, 1]\n",
    "\n",
    "    young_adults_test_with_propensity_tuned = young_adults_test.copy()\n",
    "    young_adults_test_with_propensity_tuned['membership_propensity'] = young_adults_propensity_tuned\n",
    "    young_adults_test_with_propensity_tuned['is_actual_member'] = y_test.loc[young_adults_test.index]\n",
    "\n",
    "    print(f\"\\nTotal 18-30 year olds in test set: {len(young_adults_test_with_propensity_tuned)}\")\n",
    "    print(\"\\nSample of 18-30 year olds with predicted membership propensity (Tuned Model):\")\n",
    "    print(young_adults_test_with_propensity_tuned[['age', 'online_channel', 'avg_weekly_spend_instore', 'membership_propensity', 'is_actual_member']].head(10))\n",
    "\n",
    "    propensity_threshold_tuned = young_adults_test_with_propensity_tuned['membership_propensity'].quantile(0.80)\n",
    "    high_propensity_young_adults_tuned = young_adults_test_with_propensity_tuned[young_adults_test_with_propensity_tuned['membership_propensity'] >= propensity_threshold_tuned]\n",
    "\n",
    "    print(f\"\\nNumber of 'high-propensity' 18-30 year olds (top 20% by score, Tuned Model): {len(high_propensity_young_adults_tuned)}\")\n",
    "    print(f\"Propensity threshold for 'high-propensity' 18-30 year olds (Tuned Model): {propensity_threshold_tuned:.4f}\")\n",
    "\n",
    "    print(\"\\nCharacteristics of a sample of 'high-propensity' 18-30 year olds (Tuned Model):\")\n",
    "    print(high_propensity_young_adults_tuned[['age', 'online_channel', 'avg_weekly_spend_instore', 'num_weekly_visits_instore', 'fav_product_category', 'membership_propensity', 'is_actual_member']].head())\n",
    "\n",
    "    print(\"\\nDescriptive statistics for 'high-propensity' 18-30 year olds (Tuned Model):\")\n",
    "    print(high_propensity_young_adults_tuned[['avg_page_views', 'avg_session_duration', 'num_add_to_carts', 'avg_weekly_spend_instore', 'num_weekly_visits_instore']].describe())\n",
    "    print(\"\\nMost common online channels for 'high-propensity' 18-30 year olds (Tuned Model):\")\n",
    "    print(high_propensity_young_adults_tuned['online_channel'].value_counts(normalize=True).head())\n",
    "    print(\"\\nMost common favorite product categories for 'high-propensity' 18-30 year olds (Tuned Model):\")\n",
    "    print(high_propensity_young_adults_tuned['fav_product_category'].value_counts(normalize=True).head())\n",
    "\n",
    "    print(\"\\n--- Marketing Actionable Insights (Tuned Model) ---\")\n",
    "    print(\"Based on this tuned model simulation, Co-op could refine their strategy:\")\n",
    "    print(f\"1. Target 18-30 year olds with a membership propensity score of {propensity_threshold_tuned:.2f} or higher.\")\n",
    "    print(f\"2. Focus marketing messages for this segment on benefits appealing to their likely interests (e.g., 'Ready Meals' discounts, sustainability, or community initiatives, depending on what the model's features indicate).\")\n",
    "    print(f\"3. Prioritise digital channels like '{high_propensity_young_adults_tuned['online_channel'].mode()[0]}' for reaching this segment.\")\n",
    "else:\n",
    "    print(\"No 18-30 year olds found in the test set to simulate targeting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7f487-1be8-41fb-b196-d224e7cf32ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
